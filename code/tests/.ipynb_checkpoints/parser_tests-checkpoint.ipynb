{
 "metadata": {
  "name": "",
  "signature": "sha256:8c29fbfb1366fb387e102c3331af75d00cfd84a71a22163b3d041bcaab72f189"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load ../first_link_txt.py\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#parse page xml (fed through dump)\n",
      "\n",
      "#for a list of pages: https://en.wikipedia.org/wiki/Special:AllPages\n",
      "#entire dump is https://dumps.wikimedia.org/enwiki/20141008/\n",
      "\"\"\"\n",
      "1. article body via xml tag\n",
      "2. clean tags: <ref>, <div>\n",
      "3. clean Media wiki templates {{ }}\n",
      "5. disregard parenthesis, not embedeed in links\n",
      "6. find first link \n",
      "    * elimanting outermost false links:\n",
      "        *Image, wiktionary etc.\n",
      "\"\"\"\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "#first level of hierachy is check whether {{}}\n",
      "    #proceed to ()\n",
      "    #then test link\n",
      "    #link\n",
      "\n",
      "def inside_char(char, marker, tracker, i):\n",
      "    #checks whether inside char such as parentheses or wiki_template\n",
      "        #handles nested \n",
      "    if char == marker[0]:\n",
      "        tracker.append(i)\n",
      "    elif char == marker[1]:\n",
      "        try:\n",
      "            tracker.pop()\n",
      "        except IndexError:\n",
      "            pass\n",
      "    return tracker\n",
      "        \n",
      "def grab_links(body):\n",
      "    #returns list of outer-most links not in parentheses or template or tags\n",
      "    links = []\n",
      "    link_char = []\n",
      "    \n",
      "    w_marker = [\"{{\", \"}}\"]\n",
      "    w_temp = [] #in template?\n",
      "    \n",
      "    par_marker = [\"(\", \")\"]\n",
      "    par = [] #in parentheses?\n",
      "    \n",
      "    rtag_marker = [\"<ref\", \"</re\"]\n",
      "    rtag = [] #in <ref> tag?\n",
      "    \n",
      "    dtag_marker = [\"<div\", \"</di\"]\n",
      "    dtag = []\n",
      "    \n",
      "    skip_char = []\n",
      "    \n",
      "    for i, c in enumerate(body):\n",
      "        if i in skip_char: continue #eliminates double counting\n",
      "        char = body[i:i+2]\n",
      "        tag = body[i:i+4]\n",
      "        \n",
      "        #wiki template\n",
      "        w_temp = inside_char(char, w_marker, w_temp, i)\n",
      "        if char in w_marker: skip_char.append(i+1)\n",
      "        if w_temp:\n",
      "            continue #doesn't process if inside wiki template\n",
      "        \n",
      "        #parentheses\n",
      "        par = inside_char(c, par_marker, par, i)\n",
      "        if par:\n",
      "            continue\n",
      "        \n",
      "        #<ref> or <div>\n",
      "        rtag = inside_char(tag, rtag_marker, rtag, i)\n",
      "        dtag = inside_char(tag, dtag_marker, dtag, i)\n",
      "        if rtag or dtag:\n",
      "            continue\n",
      "        \n",
      "        #clear to add outer-most link\n",
      "        if char == '[[':\n",
      "            link_char.append(i)\n",
      "        elif char == ']]' and len(link_char) == 1:\n",
      "            links.append(body[link_char[0]:i+2])\n",
      "            link_char.pop()\n",
      "        elif char == ']]' and len(link_char) > 1:\n",
      "            link_char.pop()\n",
      "    return links\n",
      "\n",
      "def check_link(link):\n",
      "    #filter links to images or files\n",
      "    #returns false if for a bad link\n",
      "        #includes links begining with colon\n",
      "    false_links = [\"wikipedia:\", \"w:\", \"wikitionary:\", \"wikt:\", \"wikinews:\",\n",
      "                    \"n:\", \"wikibooks:\", \"b:\", \"wikiquote:\", \"q:\", \"wikisource:\",\n",
      "                    \"s:\", \"wikispecies:\", \"species:\", \"wikiversity\", \"v:\", \n",
      "                    \"wikivoyage:\", \"voy:\", \"wikimedia:\", \"foundation:\", \"wmf:\", \n",
      "                    \"commonds:\", \"c:\", \"chapter:\", \"metawikipedia:\", \"meta:\", \n",
      "                    \"m:\", \"incubator:\", \"outreach:\", \"mw:\", \"mediazilla:\", \n",
      "                    \"bugzilla:\", \"testwiki:\", \"wikitech:\", \"wikidata:\", \"d:\",\n",
      "                    \"phabricator:\", \"phab:\", \"talk:\", \"user talk:\", \"file:\", \n",
      "                    \"user:\", \"template:\", \"category:\", \"file talk:\", \n",
      "                    \"category talk:\", \"image:\", \"media:\", \"special:\", \n",
      "                    \"help:\", \"portal:\", \"portal talk:\", \"\\#\"]\n",
      "    is_bad = any(false_link in link.lower() for false_link in false_links)\n",
      "    if is_bad or link[0] == \":\":\n",
      "        return False\n",
      "    else:\n",
      "        return True\n",
      "\n",
      "\n",
      "def clean_link(link):\n",
      "    #strips brackets, returns link destination (not display name)\n",
      "    link = link.strip(\"[]\")\n",
      "    if \"|\" in link:            \n",
      "        link = link.split(\"|\",1)[0]\n",
      "    link = link.strip() #remove trailing white space\n",
      "    return link \n",
      "\n",
      "def run_parser(page_xml):\n",
      "    links = grab_links(page_xml)\n",
      "    for link in links:\n",
      "        if check_link(link):\n",
      "            return clean_link(link)\n",
      "    return None\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TESTS\n",
      "import urllib2\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get live page xml\n",
      "page = \"Encyclop\u00e6dia_Britannica,_Inc.\"\n",
      "page_xml = urllib2.urlopen(\"https://en.wikipedia.org/wiki/Special:Export/\"+page).read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_parser(page_xml)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "'Encyclop\\xc3\\xa6dia Britannica (company)'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}